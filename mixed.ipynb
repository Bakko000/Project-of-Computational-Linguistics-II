{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re #pacchetto per espressioni regolari\n",
    "import os #pacchetto per muoversi nelle cartelle\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to the Python path\n",
    "from utils.helpers import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "children = pd.read_csv(\"children/children.csv\")\n",
    "children_test = pd.read_csv(\"children/children_test.csv\")\n",
    "twitter = pd.read_csv(\"twitter/twitter.csv\")\n",
    "twitter_test = pd.read_csv(\"twitter/twitter_test.csv\")\n",
    "youtube = pd.read_csv(\"youtube/youtube.csv\")\n",
    "youtube_test = pd.read_csv(\"youtube/youtube_test.csv\")\n",
    "diary = pd.read_csv(\"diary/diary.csv\")\n",
    "diary_test = pd.read_csv(\"diary/diary_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.concat([children, twitter, youtube, diary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.concat([children_test, twitter_test, youtube_test, diary_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sostituire i NaN nelle colonne \"136\" e \"137\" con 0.0\n",
    "#training_set[[\"136\", \"137\"]] = training_set[[\"136\", \"137\"]].fillna(0.0)\n",
    "#test_set[[\"136\", \"137\"]] = test_set[[\"136\", \"137\"]].fillna(0.0)\n",
    "\n",
    "# Sostituire i NaN nelle colonne con la media\n",
    "training_set['136'] = training_set['136'].fillna(training_set['136'].mean())\n",
    "training_set['137'] = training_set['137'].fillna(training_set['137'].mean())\n",
    "training_set['138'] = training_set['138'].fillna(training_set['138'].mean())\n",
    "training_set['139'] = training_set['139'].fillna(training_set['139'].mean())\n",
    "training_set['140'] = training_set['140'].fillna(training_set['140'].mean())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['136'] = test_set['136'].fillna(test_set['136'].mean())\n",
    "test_set['137'] = test_set['137'].fillna(test_set['137'].mean())\n",
    "test_set['138'] = test_set['138'].fillna(test_set['138'].mean())\n",
    "test_set['139'] = test_set['139'].fillna(test_set['139'].mean())\n",
    "test_set['140'] = test_set['140'].fillna(test_set['140'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training_set.drop(columns=['Label', 'genre']).values\n",
    "train_labels = training_set[\"Label\"].tolist()\n",
    "\n",
    "\n",
    "X_test = test_set.drop(columns=['Label', 'genre']).values\n",
    "test_labels = test_set[\"Label\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corra\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\corra\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\corra\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\corra\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\corra\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 10, 'dual': True}\n",
      "Best score found: 0.5798054654932839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\corra\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search through\n",
    "param_grid = {\n",
    "    'C': [10, 1, 0.1, 0.01, 0.001, 0.0001],  # Regularization parameter\n",
    "    'dual': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize SVM with linear kernel\n",
    "svc = LinearSVC()\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit the grid search to your data\n",
    "grid_search.fit(X_train, train_labels)\n",
    "\n",
    "# Get mean test scores across folds\n",
    "mean_test_scores = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best score found:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimator (model) found by grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Now, you can use this best_model to make predictions on new data\n",
    "# For example, if you have new data X_new, you can predict its labels as follows:\n",
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.53      0.66      0.59      5337\n",
      "           M       0.55      0.41      0.47      5337\n",
      "\n",
      "    accuracy                           0.54     10674\n",
      "   macro avg       0.54      0.54      0.53     10674\n",
      "weighted avg       0.54      0.54      0.53     10674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
    "test_predictions = predictions\n",
    "print(classification_report(test_labels, test_predictions, zero_division=0)) # output_dict=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Children_test Accuracy: 0.53\n",
      "Children_test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.53      0.55      0.54       100\n",
      "           M       0.53      0.51      0.52       100\n",
      "\n",
      "    accuracy                           0.53       200\n",
      "   macro avg       0.53      0.53      0.53       200\n",
      "weighted avg       0.53      0.53      0.53       200\n",
      "\n",
      "\n",
      "Twitter_test Accuracy: 0.5476666666666666\n",
      "Twitter_test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.54      0.66      0.59      3000\n",
      "           M       0.56      0.43      0.49      3000\n",
      "\n",
      "    accuracy                           0.55      6000\n",
      "   macro avg       0.55      0.55      0.54      6000\n",
      "weighted avg       0.55      0.55      0.54      6000\n",
      "\n",
      "\n",
      "Youtube_test Accuracy: 0.5256818181818181\n",
      "Youtube_test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.52      0.68      0.59      2200\n",
      "           M       0.54      0.37      0.44      2200\n",
      "\n",
      "    accuracy                           0.53      4400\n",
      "   macro avg       0.53      0.53      0.51      4400\n",
      "weighted avg       0.53      0.53      0.51      4400\n",
      "\n",
      "\n",
      "Diary_test Accuracy: 0.527027027027027\n",
      "Diary_test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.53      0.49      0.51        37\n",
      "           M       0.53      0.57      0.55        37\n",
      "\n",
      "    accuracy                           0.53        74\n",
      "   macro avg       0.53      0.53      0.53        74\n",
      "weighted avg       0.53      0.53      0.53        74\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Aggiungi le predizioni come una nuova colonna nel test_set\n",
    "test_set['predicted'] = test_predictions\n",
    "\n",
    "# Lista delle categorie da analizzare\n",
    "genres = ['children_test', 'twitter_test', 'youtube_test', 'diary_test']\n",
    "\n",
    "# Ciclo per calcolare le metriche per ogni categoria\n",
    "for genre in genres:\n",
    "    # Filtra per categoria\n",
    "    genre_data = test_set[test_set['genre'] == genre]\n",
    "    genre_labels = genre_data['Label']\n",
    "    genre_predictions = genre_data['predicted']\n",
    "    \n",
    "    # Calcola le metriche per la categoria\n",
    "    accuracy = accuracy_score(genre_labels, genre_predictions)\n",
    "    report = classification_report(genre_labels, genre_predictions)\n",
    "    \n",
    "    # Mostra i risultati\n",
    "    print(f\"{genre.capitalize()} Accuracy: {accuracy}\")\n",
    "    print(f\"{genre.capitalize()} Classification Report:\\n{report}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
