{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "dim = 64   #32 o 64\n",
    "dir = 'itwac'\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "sql_path = f'{parent_dir}/{dir}/itwac{dim}.sqlite'  #(1) best\n",
    "txt_path = f'{parent_dir}/{dir}/itwac{dim}.txt'\n",
    "conllu_dir = 'data/profiling_output/11459'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\corra\\\\Documents\\\\GitHub\\\\ProgettoLinCompII\\\\Project-of-Computational-Linguistics-II/itwac/itwac64.sqlite'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dim = 64\n",
    "embeddings_path = f'itwac/itwac{embeddings_dim}.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')  # Add parent directory to the Python path\n",
    "from utils.helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_word_embeddings(parent_dir+\"/\"+embeddings_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.21470731, -0.1392023 ,  0.27303648, -0.05340238, -0.03936952,\n",
       "        0.02390084, -0.02741197, -0.12491986, -0.02866316,  0.00107179,\n",
       "       -0.09525209,  0.05685699,  0.01190181, -0.09539247,  0.06602568,\n",
       "       -0.0245655 , -0.09080959,  0.10820474, -0.04021769,  0.01245855,\n",
       "        0.19386294, -0.17447612, -0.03580143,  0.14230289,  0.22230086,\n",
       "        0.00798338, -0.07439804,  0.06270457, -0.00456899, -0.37990505,\n",
       "        0.06431432,  0.00839787, -0.06637963,  0.06906799, -0.30072612,\n",
       "        0.27667612, -0.06807792, -0.09943178, -0.10579097,  0.02905671,\n",
       "        0.20786461,  0.01789608, -0.08334571,  0.01347961,  0.06287382,\n",
       "        0.01383804, -0.00558291, -0.14960463, -0.12520191,  0.16182758,\n",
       "        0.07295152,  0.01592852, -0.01849817,  0.0850869 ,  0.04188532,\n",
       "       -0.05708888,  0.0698403 ,  0.02712907,  0.07868258, -0.01340355,\n",
       "        0.14947875, -0.33357722, -0.07842311,  0.02836373])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings['veder-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "all_documents_paths = []\n",
    "for file_name in os.listdir(conllu_dir):\n",
    "    file_path = os.path.join(conllu_dir, file_name)\n",
    "    all_documents_paths.append(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = []\n",
    "\n",
    "for document_path in all_documents_paths:\n",
    "    document_tokens = get_tokens_from_file(document_path)\n",
    "    all_documents.append(document_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings_mean(document_embeddings):\n",
    "    sum_array = np.sum(document_embeddings, axis=0)\n",
    "    mean_array = np.divide(sum_array, len(document_embeddings))\n",
    "    return mean_array\n",
    "\n",
    "def compute_all_embeddings_mean(document_tokens):\n",
    "    document_embeddings = []\n",
    "    \n",
    "    for token in document_tokens:\n",
    "        word = token['word']\n",
    "        if word in embeddings:\n",
    "            document_embeddings.append(embeddings[word])\n",
    "    \n",
    "    if len(document_embeddings) == 0:\n",
    "        mean_document_embeddings = np.zeros(embeddings_dim)\n",
    "    else:\n",
    "        mean_document_embeddings = compute_embeddings_mean(document_embeddings)\n",
    "    return mean_document_embeddings\n",
    "\n",
    "\n",
    "def compute_filtered_embeddings_mean(document_tokens):\n",
    "    document_embeddings = []\n",
    "    \n",
    "    for token in document_tokens:\n",
    "        word = token['word']\n",
    "        pos = token['pos']\n",
    "        if word in embeddings and pos in ['ADJ', 'NOUN', 'VERB']:\n",
    "            document_embeddings.append(embeddings[word])\n",
    "    \n",
    "    if len(document_embeddings) == 0:\n",
    "        mean_document_embeddings = np.zeros(embeddings_dim)\n",
    "    else:\n",
    "        mean_document_embeddings = compute_embeddings_mean(document_embeddings)\n",
    "    return mean_document_embeddings\n",
    "\n",
    "\n",
    "def compute_filtered_embeddings_sep_means(document_tokens):\n",
    "    adj_embeddings = []\n",
    "    noun_embeddings = []\n",
    "    verb_embeddings = []\n",
    "    \n",
    "    for token in document_tokens:\n",
    "        word = token['word']\n",
    "        pos = token['pos']\n",
    "        if word in embeddings and pos in ['ADJ']:\n",
    "            adj_embeddings.append(embeddings[word])\n",
    "        elif word in embeddings and pos in ['NOUN']:\n",
    "            noun_embeddings.append(embeddings[word])\n",
    "        elif word in embeddings and pos in ['VERB']:\n",
    "            verb_embeddings.append(embeddings[word])\n",
    "    \n",
    "    if len(adj_embeddings) == 0:\n",
    "        mean_adj_embeddings = np.zeros(embeddings_dim)\n",
    "    else:\n",
    "        mean_adj_embeddings = compute_embeddings_mean(adj_embeddings)\n",
    "        \n",
    "    if len(noun_embeddings) == 0:\n",
    "        mean_noun_embeddings = np.zeros(embeddings_dim)\n",
    "    else:\n",
    "        mean_noun_embeddings = compute_embeddings_mean(noun_embeddings)\n",
    "        \n",
    "    if len(verb_embeddings) == 0:\n",
    "        mean_verb_embeddings = np.zeros(embeddings_dim)\n",
    "    else:\n",
    "        mean_verb_embeddings = compute_embeddings_mean(verb_embeddings)  \n",
    "    \n",
    "    \n",
    "    mean_document_embeddings = np.concatenate([mean_adj_embeddings, mean_noun_embeddings, mean_verb_embeddings], axis=None)\n",
    "    return mean_document_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(documents):\n",
    "    dataset_features = []\n",
    "    for document_tokens in documents:\n",
    "        document_embeddings = compute_all_embeddings_mean(document_tokens)\n",
    "        # document_embeddings = compute_filtered_embeddings_mean(document_tokens)\n",
    "        # document_embeddings = compute_filtered_embeddings_sep_means(document_tokens)\n",
    "        dataset_features.append(document_embeddings)\n",
    "    return dataset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = extract_features(all_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274, 64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_features), len(all_features[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = create_label_list(all_documents_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(all_features, all_labels, all_documents_paths):\n",
    "    train_features, train_labels = [], []\n",
    "    test_features, test_labels = [], []\n",
    "    \n",
    "    for features, label,  document_path in zip(all_features, all_labels, all_documents_paths):\n",
    "        if 'training' in document_path:\n",
    "            train_features.append(features)\n",
    "            train_labels.append(label)\n",
    "        else:\n",
    "            test_features.append(features)\n",
    "            test_labels.append(label)\n",
    "    return train_features, train_labels, test_features, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200, 74, 74)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, train_labels, test_features, test_labels = train_test_split(all_features, all_labels, all_documents_paths)\n",
    "len(train_features), len(train_labels), len(test_features), len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# X_train = np.stack(train_features, axis=0)\n",
    "X_train = scaler.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 0.1, 'dual': True}\n",
      "Best score found: 0.6699999999999999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Define the parameter grid to search through\n",
    "param_grid = {\n",
    "    'C': [0.1, 0.01, 0.001],  # Regularization parameter\n",
    "    'dual': [True, False]\n",
    "}\n",
    "\n",
    "# Initialize SVM with linear kernel\n",
    "svc = LinearSVC()\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=5, refit=True)\n",
    "\n",
    "# Fit the grid search to your data\n",
    "grid_search.fit(X_train, train_labels)\n",
    "\n",
    "# Get mean test scores across folds\n",
    "mean_test_scores = grid_search.cv_results_['mean_test_score']\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "print(\"Best score found:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = scaler.transform(test_features)\n",
    "# Get the best estimator (model) found by grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Now, you can use this best_model to make predictions on new data\n",
    "# For example, if you have new data X_new, you can predict its labels as follows:\n",
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           F       0.79      0.70      0.74        37\n",
      "           M       0.73      0.81      0.77        37\n",
      "\n",
      "    accuracy                           0.76        74\n",
      "   macro avg       0.76      0.76      0.76        74\n",
      "weighted avg       0.76      0.76      0.76        74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay, confusion_matrix\n",
    "test_predictions = predictions\n",
    "print(classification_report(test_labels, test_predictions, zero_division=0)) # output_dict=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
